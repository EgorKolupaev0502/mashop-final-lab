{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc26af8-f603-466a-a5e4-c76c53985e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "from keras._tf_keras import keras\n",
    "from navec import Navec\n",
    "\n",
    "from scipy.stats import gmean\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier, LogisticRegressionCV\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from keras import Sequential\n",
    "from keras.src.layers import Embedding\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils.data_loader import load_file, load_files\n",
    "from utils.preprocessing import preprocess_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2d4219-b4eb-4b3d-bc8c-9b594edc96ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Загрузка корпусов текстов. Разбиваем на предложения и метки.\n",
    "\n",
    "sentences, aspects_marks = load_files(\n",
    "                                      \"data/aspects-marked-part-2-elections-1.csv\",\n",
    "                                      \"data/aspects-marked-part-2-elections-2.csv\",\n",
    "                                      \"data/aspects-marked-part-2-elections-3.csv\"\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0737d6b9-a325-470e-933a-6526369fce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Классификаторы\n",
    "\n",
    "classifiers_builders = [\n",
    "    lambda: RidgeClassifier(class_weight=\"balanced\"),\n",
    "    lambda: LogisticRegression(class_weight=\"balanced\"),\n",
    "    lambda: SVC(class_weight=\"balanced\", random_state=42),\n",
    "    lambda: LinearSVC(class_weight=\"balanced\", random_state=42),\n",
    "    lambda: SGDClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    lambda: DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    lambda: RandomForestClassifier(class_weight=\"balanced\", random_state=42, max_depth=3),\n",
    "    lambda : LogisticRegressionCV(class_weight=\"balanced\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af53db48-dd87-48b7-addf-5694a71f4e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Начинаем с Glove(navec)\n",
    "\n",
    "# Объевляем медели \n",
    "\n",
    "models = [\n",
    "          'models/navec_hudlit_v1_12B_500K_300d_100q.tar',\n",
    "          'models/navec_news_v1_1B_250K_300d_100q.tar'\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beea0c95-6253-4caa-9fea-e10e995cdc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Гиперпараметры:\n",
    "# лемматизация\n",
    "# стемминг\n",
    "# минимальная длина слова (потом уточню значения) **возможно заменим на удаление стоп слов\n",
    "\n",
    "lemmatize = [True, False]\n",
    "stem = [True, False]\n",
    "min_word_len = [0, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4888d014-6912-4dfa-9d29-8e4683db8c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение эмбеддинга предложения усреднением эмбэддинга слов\n",
    "\n",
    "def apply_navec(sentence, model_):\n",
    "    unk = model['<unk>']\n",
    "    words_matrix = np.array([model_.get(word, unk) for word in sentence])\n",
    "    word = words_matrix.mean(axis=0)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a091eee9-93d9-4bcd-8edb-16090ab3dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение эмбеддинга предложения прогоном эмбеддинга слов через слой lstm\n",
    "# Гиперпараметры: размер выходного вектора, коэффицент дропаута\n",
    "\n",
    "LSTM_SIZE = 1024\n",
    "coef_dropout = 0.5\n",
    "lstm = keras.layers.LSTM(LSTM_SIZE, dropout=coef_dropout, recurrent_dropout=coef_dropout)\n",
    "def apply_navec_with_lstm(sentence, model_):\n",
    "    unk = model['<unk>']\n",
    "    words_matrix = np.array([model_.get(word, unk) for word in sentence])\n",
    "    words_vectors = []\n",
    "    words_vectors.append(words_matrix)\n",
    "    words_vectors = np.array(words_vectors)\n",
    "    output = lstm(words_vectors)\n",
    "    words_vectors_lstm = np.array(output[0])\n",
    "    return words_vectors_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb2a3f5-05a1-441f-b664-d568846cca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Описание вывода\n",
    "\n",
    "best_f1, best_prec, best_rec = 0, 0, 0\n",
    "\n",
    "results, results_header = [], [\"model\", \"lemm\", \"stem\", \"min_word_len\", \"classifier\", \"avg_prec\", \"avg_rec\", \"avg_f1\"]\n",
    "result_console = []\n",
    "\n",
    "dict_sentence = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00787944-07eb-4b73-8e64-9f9a349841ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Полный эксперимент через усреднение эмбеддинга\n",
    "# Разобью позже\n",
    "\n",
    "for model_path in models:\n",
    "    model = Navec.load(model_path)\n",
    "    for lemmatize_, stem_, min_word_len_, in product(lemmatize, stem, min_word_len):\n",
    "        print(lemmatize_, stem_, min_word_len_)\n",
    "        try:\n",
    "            X = np.array([apply_navec(preprocess_sentence(sentence, lemmatize_, min_word_len_, stem_, True), model) for sentence in sentences])\n",
    "        except KeyError:\n",
    "            print(model_path, lemmatize_, stem_, min_word_len_, \"failed\")\n",
    "            results.append([model_path, lemmatize_, stem_, min_word_len_, \"\", None, None, None])\n",
    "            continue\n",
    "        for classifier_ in classifiers_builders:\n",
    "            classifier = classifier_()\n",
    "            result_console.append(f\"{model_path} {classifier.__class__} {lemmatize_} {stem_} {min_word_len_}\\n\")\n",
    "            f1_scores, precisions, recalls = [], [], []\n",
    "            for aspect in aspects_marks.columns:\n",
    "                y = aspects_marks[aspect].values.ravel()\n",
    "                if y.sum() < 50:\n",
    "                    continue\n",
    "                y_pred = np.array([0] * y.shape[0])\n",
    "                y_train_pred = np.array([0] * y.shape[0])\n",
    "                for train_ix, test_ix in StratifiedKFold(n_splits=5).split(X, y):\n",
    "                    X_train, X_test, y_train, y_test = X[train_ix], X[test_ix], y[train_ix], y[test_ix]\n",
    "                    clf = classifier\n",
    "                    clf.fit(X_train, y_train)\n",
    "                    y_train_out = clf.predict(X_train)\n",
    "                    y_test_out = clf.predict(X_test)\n",
    "                    y_train_pred[train_ix] = y_train_out\n",
    "                    y_pred[test_ix] = y_test_out\n",
    "                prec, rec, f1, support = [_[0] for _ in precision_recall_fscore_support(y, y_pred, labels=[1])]\n",
    "                train_f1 = f1_score(y, y_train_pred, labels=[1])\n",
    "                f1_scores.append(f1)\n",
    "                precisions.append(prec)\n",
    "                recalls.append(rec)\n",
    "                result_console.append(f\"Упоминание аспекта '{aspect:30}' ({support:3}): \"\n",
    "                      f\"precision {prec:.2f}, recall {rec:.2f}, F1 {f1:.2f} (при обучении {train_f1:.2f})\\n\")\n",
    "\n",
    "            avg_f1 = gmean(f1_scores).ravel()[0]\n",
    "            avg_prec = gmean(precisions).ravel()[0]\n",
    "            avg_rec = gmean(recalls).ravel()[0]\n",
    "            results.append([model_path, lemmatize_, stem_, min_word_len_, classifier.__class__, avg_prec, avg_rec, avg_f1])\n",
    "            tmp = list(map(lambda x: round(x, 3), [avg_prec, avg_rec, avg_f1]))\n",
    "            for tmp1 in tmp:\n",
    "                result_console.append(str(tmp1) + \" \")\n",
    "            if avg_f1 > best_f1:\n",
    "                best_f1, best_prec, best_rec = avg_f1, avg_prec, avg_rec\n",
    "            result_console.append(\"\\n\\n\")\n",
    "\n",
    "tmp = list(map(lambda x: round(x, 3), [best_prec, best_rec, best_f1]))\n",
    "for tmp1 in tmp:\n",
    "    result_console.append(str(tmp1) + \" \")\n",
    "\n",
    "with open(f\"journal/GloVe(navec)-result-{datetime.now().date().strftime('%d%m%Y')}.csv\", 'w', encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(results_header)\n",
    "    writer.writerows(results)\n",
    "\n",
    "file = open(f\"journal/GloVe(navec)-result-{datetime.now().date().strftime('%d%m%Y')}.txt\", \"w\")\n",
    "for str in result_console:\n",
    "    file.write(str)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f3fd4a-5504-4c2f-8120-12ebba8dc9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эксперимент с lstm\n",
    "# Аналогично разобью потом\n",
    "\n",
    "for model_path in models:\n",
    "    model = Navec.load(model_path)\n",
    "    for lemmatize_, stem_, min_word_len_, in product(lemmatize, stem, min_word_len):\n",
    "        print(lemmatize_, stem_, min_word_len_)\n",
    "        try:\n",
    "            X = np.array([apply_navec_with_lstm(preprocess_sentence(sentence, lemmatize_, min_word_len_, stem_, True), model) for sentence in sentences])\n",
    "        except KeyError:\n",
    "            print(model_path, lemmatize_, stem_, min_word_len_, \"failed\")\n",
    "            results.append([model_path, lemmatize_, stem_, min_word_len_, \"\", None, None, None])\n",
    "            continue\n",
    "        for classifier_ in classifiers_builders:\n",
    "            classifier = classifier_()\n",
    "            result_console.append(f\"{model_path} {classifier.__class__} {lemmatize_} {stem_} {min_word_len_}\\n\")\n",
    "            f1_scores, precisions, recalls = [], [], []\n",
    "            for aspect in aspects_marks.columns:\n",
    "                y = aspects_marks[aspect].values.ravel()\n",
    "                if y.sum() < 50:\n",
    "                    continue\n",
    "                y_pred = np.array([0] * y.shape[0])\n",
    "                y_train_pred = np.array([0] * y.shape[0])\n",
    "                for train_ix, test_ix in StratifiedKFold(n_splits=5).split(X, y):\n",
    "                    X_train, X_test, y_train, y_test = X[train_ix], X[test_ix], y[train_ix], y[test_ix]\n",
    "                    clf = classifier\n",
    "                    clf.fit(X_train, y_train)\n",
    "                    y_train_out = clf.predict(X_train)\n",
    "                    y_test_out = clf.predict(X_test)\n",
    "                    y_train_pred[train_ix] = y_train_out\n",
    "                    y_pred[test_ix] = y_test_out\n",
    "                prec, rec, f1, support = [_[0] for _ in precision_recall_fscore_support(y, y_pred, labels=[1])]\n",
    "                train_f1 = f1_score(y, y_train_pred, labels=[1])\n",
    "                f1_scores.append(f1)\n",
    "                precisions.append(prec)\n",
    "                recalls.append(rec)\n",
    "                result_console.append(f\"Упоминание аспекта '{aspect:30}' ({support:3}): \"\n",
    "                      f\"precision {prec:.2f}, recall {rec:.2f}, F1 {f1:.2f} (при обучении {train_f1:.2f})\\n\")\n",
    "\n",
    "            avg_f1 = gmean(f1_scores).ravel()[0]\n",
    "            avg_prec = gmean(precisions).ravel()[0]\n",
    "            avg_rec = gmean(recalls).ravel()[0]\n",
    "            results.append([model_path, lemmatize_, stem_, min_word_len_, classifier.__class__, avg_prec, avg_rec, avg_f1])\n",
    "            tmp = list(map(lambda x: round(x, 3), [avg_prec, avg_rec, avg_f1]))\n",
    "            for tmp1 in tmp:\n",
    "                result_console.append(str(tmp1) + \" \")\n",
    "            if avg_f1 > best_f1:\n",
    "                best_f1, best_prec, best_rec = avg_f1, avg_prec, avg_rec\n",
    "            result_console.append(\"\\n\\n\")\n",
    "\n",
    "tmp = list(map(lambda x: round(x, 3), [best_prec, best_rec, best_f1]))\n",
    "for tmp1 in tmp:\n",
    "    result_console.append(str(tmp1) + \" \")\n",
    "\n",
    "with open(f\"journal/GloVe(navec)-lstm-{LSTM_SIZE}-{coef_dropout}-result-{datetime.now().date().strftime('%d%m%Y')}.csv\", 'w', encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(results_header)\n",
    "    writer.writerows(results)\n",
    "\n",
    "file = open(f\"journal/GloVe(navec)-lstm-{LSTM_SIZE}-{coef_dropout}-result-{datetime.now().date().strftime('%d%m%Y')}.txt\", \"w\")\n",
    "for str in result_console:\n",
    "    file.write(str)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ae186b-0d56-4cad-a6b0-b2cc8a3d78b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Далее сделаем Fasttext\n",
    "def apply_fasttext(sentence, model_: FastText):\n",
    "    words_vectors = np.array([model_.wv.get_vector(word) for word in sentence])\n",
    "    print(len(words_vectors[0]))\n",
    "    return words_vectors.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29422e6-4221-48f7-9470-920402e18e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lemmatize_, stem_, min_word_len_, in product(lemmatize, stem, min_word_len):\n",
    "        print(lemmatize_, stem_, min_word_len_)\n",
    "        try:\n",
    "            preprocess = partial(preprocess_sentence,\n",
    "                                 lemmatize=lemmatize_,\n",
    "                                 stem=stem_,\n",
    "                                 min_word_len=min_word_len_)\n",
    "            preprocessed_sentences = sentences.map(preprocess_sentence)\n",
    "            model = FastText(vector_size=512,\n",
    "                             window=6,\n",
    "                             min_count=0)\n",
    "            model.build_vocab(corpus_iterable=preprocessed_sentences)\n",
    "            model.train(corpus_iterable=preprocessed_sentences,\n",
    "                        total_examples=len(preprocessed_sentences),\n",
    "                        epochs=150)\n",
    "            X = np.array([apply_fasttext(sentence, model) for sentence in preprocessed_sentences])\n",
    "        except KeyError:\n",
    "            print( lemmatize_, stem_, min_word_len_, \"failed\")\n",
    "            results.append([ lemmatize_, stem_, min_word_len_, \"\", None, None, None])\n",
    "            continue\n",
    "        for classifier_ in classifiers_builders:\n",
    "            print(1)\n",
    "            classifier = classifier_()\n",
    "            result_console.append(f\" {classifier.__class__} {lemmatize_} {stem_} {min_word_len_}\\n\")\n",
    "            f1_scores, precisions, recalls = [], [], []\n",
    "            for aspect in aspects_marks.columns:\n",
    "                y = aspects_marks[aspect].values.ravel()\n",
    "                if y.sum() < 50:\n",
    "                    continue\n",
    "                y_pred = np.array([0] * y.shape[0])\n",
    "                y_train_pred = np.array([0] * y.shape[0])\n",
    "                for train_ix, test_ix in StratifiedKFold(n_splits=5).split(X, y):\n",
    "                    X_train, X_test, y_train, y_test = X[train_ix], X[test_ix], y[train_ix], y[test_ix]\n",
    "                    clf = classifier\n",
    "                    clf.fit(X_train, y_train)\n",
    "                    y_train_out = clf.predict(X_train)\n",
    "                    y_test_out = clf.predict(X_test)\n",
    "                    y_train_pred[train_ix] = y_train_out\n",
    "                    y_pred[test_ix] = y_test_out\n",
    "                prec, rec, f1, support = [_[0] for _ in precision_recall_fscore_support(y, y_pred, labels=[1])]\n",
    "                train_f1 = f1_score(y, y_train_pred, labels=[1])\n",
    "                f1_scores.append(f1)\n",
    "                precisions.append(prec)\n",
    "                recalls.append(rec)\n",
    "                result_console.append(f\"Упоминание аспекта '{aspect:30}' ({support:3}): \"\n",
    "                      f\"precision {prec:.2f}, recall {rec:.2f}, F1 {f1:.2f} (при обучении {train_f1:.2f})\\n\")\n",
    "\n",
    "            avg_f1 = gmean(f1_scores).ravel()[0]\n",
    "            avg_prec = gmean(precisions).ravel()[0]\n",
    "            avg_rec = gmean(recalls).ravel()[0]\n",
    "            results.append([lemmatize_, stem_, min_word_len_, classifier.__class__, avg_prec, avg_rec, avg_f1])\n",
    "            tmp = list(map(lambda x: round(x, 3), [avg_prec, avg_rec, avg_f1]))\n",
    "            for tmp1 in tmp:\n",
    "                result_console.append(str(tmp1) + \" \")\n",
    "            if avg_f1 > best_f1:\n",
    "                best_f1, best_prec, best_rec = avg_f1, avg_prec, avg_rec\n",
    "            result_console.append(\"\\n\\n\")\n",
    "\n",
    "tmp = list(map(lambda x: round(x, 3), [best_prec, best_rec, best_f1]))\n",
    "for tmp1 in tmp:\n",
    "    result_console.append(str(tmp1) + \" \")\n",
    "\n",
    "\n",
    "with open(f\"journal/FastText-result-{datetime.now().date().strftime('%d%m%Y')}.csv\", 'w', encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(results_header)\n",
    "    writer.writerows(results)\n",
    "\n",
    "file = open(f\"journal/FastText-result-{datetime.now().date().strftime('%d%m%Y')}.txt\", \"w\")\n",
    "for str in result_console:\n",
    "    file.write(str)\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
